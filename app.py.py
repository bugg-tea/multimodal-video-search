# -*- coding: utf-8 -*-
"""final-video-text-ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12JPFB8esbw7d3D5U1xJKQ08uzX1N0FCD
"""

!pip install -q moviepy opencv-python-headless pillow tqdm openai-whisper transformers sentencepiece sentence-transformers faiss-cpu git+https://github.com/openai/CLIP.git accelerate
from google.colab import drive
drive.mount('/content/drive')

import os, json, subprocess, time
from tqdm import tqdm
from PIL import Image
import cv2
import numpy as np
import torch

import whisper
from transformers import BlipProcessor, BlipForConditionalGeneration
from sentence_transformers import SentenceTransformer
import faiss

# ------------------------------
# Folders
# ------------------------------
ROOT = "/content"
VIDEO_FOLDER = os.path.join(ROOT, "drive", "MyDrive", "videos")
CHUNK_FOLDER = os.path.join(ROOT, "chunks")
FRAME_FOLDER = os.path.join(ROOT, "frames")
TRANSCRIPT_FOLDER = os.path.join(ROOT, "transcripts")
EMBEDDING_FOLDER = os.path.join(ROOT, "embeddings")
INDEX_FOLDER = os.path.join(ROOT, "indexes")

for p in [CHUNK_FOLDER, FRAME_FOLDER, TRANSCRIPT_FOLDER, EMBEDDING_FOLDER, INDEX_FOLDER]:
    os.makedirs(p, exist_ok=True)

# ------------------------------
# Parameters
# ------------------------------
CHUNK_DURATION = 5   # seconds per chunk
HOP = 3              # overlap
NUM_FRAMES = 12
TEXT_WEIGHT = 0.4
VISUAL_WEIGHT = 0.6
TOP_K = 5
AUDIO_SIM_THRESHOLD = 0.25
VISUAL_SIM_THRESHOLD = 0.28

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# File paths
CHUNK_META_FILE = os.path.join(CHUNK_FOLDER, "chunk_metadata.json")
EMB_AUDIO_FILE = os.path.join(EMBEDDING_FOLDER, "emb_audio.json")
EMB_VISUAL_FILE = os.path.join(EMBEDDING_FOLDER, "emb_visual.json")
AUDIO_INDEX_FILE = os.path.join(INDEX_FOLDER, "faiss_audio.index")
VISUAL_INDEX_FILE = os.path.join(INDEX_FOLDER, "faiss_visual.index")
AUDIO_META_FILE = os.path.join(INDEX_FOLDER, "audio_meta.json")
VISUAL_META_FILE = os.path.join(INDEX_FOLDER, "visual_meta.json")

def run_cmd(cmd):
    return subprocess.check_output(cmd, shell=True).decode("utf-8")

def get_duration(path):
    try:
        out = run_cmd(f'ffprobe -v quiet -print_format json -show_format "{path}"')
        info = json.loads(out)
        return float(info["format"]["duration"])
    except:
        return None

def save_json(path, obj):
    with open(path, "w") as f:
        json.dump(obj, f, indent=2)

def load_json(path):
    if os.path.exists(path):
        with open(path, "r") as f:
            return json.load(f)
    return None

chunks_meta = load_json(CHUNK_META_FILE) or []

video_list = sorted([f for f in os.listdir(VIDEO_FOLDER) if f.lower().endswith((".mp4",".mkv",".avi",".mov"))])
video_list = video_list[:5]  # optional for testing

existing_video_ids = set([c["video_id"] for c in chunks_meta])

for video_name in tqdm(video_list, desc="Chunking videos"):
    video_path = os.path.join(VIDEO_FOLDER, video_name)
    video_id = os.path.splitext(video_name)[0]
    if video_id in existing_video_ids:
        continue
    dur = get_duration(video_path)
    if dur is None:
        print("Skipping:", video_name)
        continue

    out_dir = os.path.join(CHUNK_FOLDER, video_id)
    os.makedirs(out_dir, exist_ok=True)

    start = 0.0
    while start < dur:
        end = min(start + CHUNK_DURATION, dur)
        chunk_name = f"{int(start)}_{int(end)}.mp4"
        chunk_path = os.path.join(out_dir, chunk_name)
        if not os.path.exists(chunk_path):
            try:
                run_cmd(f'ffmpeg -y -i "{video_path}" -ss {start} -to {end} -c copy "{chunk_path}"')
            except Exception as e:
                print("ffmpeg failed:", e)
        chunks_meta.append({
            "video": video_name,
            "video_id": video_id,
            "chunk_file": chunk_path,
            "start": start,
            "end": end
        })
        start += HOP

    save_json(CHUNK_META_FILE, chunks_meta)
    existing_video_ids.add(video_id)

print("Chunks created:", len(chunks_meta))

for chunk in tqdm(chunks_meta, desc="Extracting frames"):
    if chunk.get("frame_paths") and len(chunk["frame_paths"]) >= NUM_FRAMES:
        continue
    chunk_path = chunk["chunk_file"]
    video_id = chunk["video_id"]
    frame_dir = os.path.join(FRAME_FOLDER, video_id)
    os.makedirs(frame_dir, exist_ok=True)
    base = os.path.splitext(os.path.basename(chunk_path))[0]

    cap = cv2.VideoCapture(chunk_path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total <= 1:
        chunk["frame_paths"] = []
        cap.release()
        save_json(CHUNK_META_FILE, chunks_meta)
        continue

    idxs = np.linspace(0, total-1, NUM_FRAMES, dtype=int)
    saved = []
    for i, fidx in enumerate(idxs):
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(fidx))
        ret, frame = cap.read()
        if not ret:
            continue
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        path = os.path.join(frame_dir, f"{base}_f{i}.jpg")
        Image.fromarray(img).save(path)
        saved.append(path)
    cap.release()
    chunk["frame_paths"] = saved
    save_json(CHUNK_META_FILE, chunks_meta)

print("Keyframes extracted.")

print("Loading models... this may take a while")
# Whisper for audio transcription
import whisper
whisper_model = whisper.load_model("medium")

# BLIP for visual captions
from transformers import BlipProcessor, BlipForConditionalGeneration
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# SentenceTransformer for embeddings
from sentence_transformers import SentenceTransformer
text_model = SentenceTransformer('all-mpnet-base-v2', device=device)
print("Models loaded.")

!pip uninstall -y whisper
!pip uninstall -y openai-whisper

!pip install git+https://github.com/openai/whisper.git

import moviepy.editor as mp

for chunk in tqdm(chunks_meta, desc="Audio transcription"):
    if "audio_transcript" in chunk:
        continue
    chunk_path = chunk["chunk_file"]
    audio_path = chunk_path.replace(".mp4", ".wav")

    try:
        if not os.path.exists(audio_path):
            clip = mp.VideoFileClip(chunk_path)
            if clip.audio is None:
                chunk["audio_transcript"] = ""
                save_json(CHUNK_META_FILE, chunks_meta)
                continue
            clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
            clip.reader.close()
            if clip.audio:
                clip.audio.reader.close_proc()
    except:
        chunk["audio_transcript"] = ""
        save_json(CHUNK_META_FILE, chunks_meta)
        continue

    try:
        res = whisper_model.transcribe(audio_path, fp16=False, language="en")
        chunk["audio_transcript"] = res.get("text", "").strip()
    except:
        chunk["audio_transcript"] = ""
    save_json(CHUNK_META_FILE, chunks_meta)

for chunk in tqdm(chunks_meta, desc="Visual captioning"):
    if chunk.get("visual_captions") and len(chunk["visual_captions"]) >= 1:
        continue
    captions = []
    for fp in chunk.get("frame_paths", []):
        try:
            image = Image.open(fp).convert("RGB")
            inputs = blip_processor(images=image, return_tensors="pt").to(device)
            with torch.no_grad():
                out = blip_model.generate(**inputs, max_length=32)
            caption = blip_processor.decode(out[0], skip_special_tokens=True)
            captions.append({
                "frame_path": fp,
                "caption": caption,
                "timestamp": chunk.get("start", 0.0)
            })
        except:
            continue
    chunk["visual_captions"] = captions
    save_json(CHUNK_META_FILE, chunks_meta)

emb_audio = load_json(EMB_AUDIO_FILE) or []
emb_visual = load_json(EMB_VISUAL_FILE) or []

existing_audio = set(e["chunk_file"] for e in emb_audio)
existing_visual = set(e["frame_path"] for e in emb_visual)

# Audio embeddings
for chunk in tqdm(chunks_meta, desc="Embedding audio"):
    if chunk["chunk_file"] in existing_audio:
        continue
    txt = chunk.get("audio_transcript","")
    emb = text_model.encode(txt).astype("float32")
    if np.linalg.norm(emb)>0: emb/=np.linalg.norm(emb)
    emb_audio.append({
        "video": chunk["video"],
        "video_id": chunk["video_id"],
        "chunk_file": chunk["chunk_file"],
        "start": chunk["start"],
        "end": chunk["end"],
        "transcript": txt,
        "embedding": emb.tolist()
    })
    existing_audio.add(chunk["chunk_file"])
    save_json(EMB_AUDIO_FILE, emb_audio)

# Visual embeddings
for chunk in tqdm(chunks_meta, desc="Embedding visual"):
    for vc in chunk.get("visual_captions", []):
        fp = vc["frame_path"]
        if fp in existing_visual:
            continue
        cap_text = vc.get("caption","")
        emb = text_model.encode(cap_text).astype("float32")
        if np.linalg.norm(emb)>0: emb/=np.linalg.norm(emb)
        emb_visual.append({
            "video": chunk["video"],
            "video_id": chunk["video_id"],
            "chunk_file": chunk["chunk_file"],
            "start": chunk["start"],
            "end": chunk["end"],
            "timestamp": vc.get("timestamp"),
            "frame_path": fp,
            "caption": cap_text,
            "embedding": emb.tolist()
        })
        existing_visual.add(fp)
        save_json(EMB_VISUAL_FILE, emb_visual)

# Audio index
if os.path.exists(AUDIO_INDEX_FILE) and os.path.exists(AUDIO_META_FILE):
    audio_index = faiss.read_index(AUDIO_INDEX_FILE)
    audio_meta = load_json(AUDIO_META_FILE)
else:
    if len(emb_audio)==0:
        audio_index, audio_meta = None, []
    else:
        A = np.array([np.array(x["embedding"],dtype="float32") for x in emb_audio])
        audio_index = faiss.IndexFlatIP(A.shape[1])
        audio_index.add(A)
        faiss.write_index(audio_index, AUDIO_INDEX_FILE)
        audio_meta = emb_audio.copy()
        save_json(AUDIO_META_FILE, audio_meta)

# Visual index
if os.path.exists(VISUAL_INDEX_FILE) and os.path.exists(VISUAL_META_FILE):
    visual_index = faiss.read_index(VISUAL_INDEX_FILE)
    visual_meta = load_json(VISUAL_META_FILE)
else:
    if len(emb_visual)==0:
        visual_index, visual_meta = None, []
    else:
        V = np.array([np.array(x["embedding"],dtype="float32") for x in emb_visual])
        visual_index = faiss.IndexFlatIP(V.shape[1])
        visual_index.add(V)
        faiss.write_index(visual_index, VISUAL_INDEX_FILE)
        visual_meta = emb_visual.copy()
        save_json(VISUAL_META_FILE, visual_meta)

def cosine_score(a,b):
    a = np.array(a,dtype="float32"); b=np.array(b,dtype="float32")
    if np.linalg.norm(a)==0 or np.linalg.norm(b)==0: return 0.0
    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))

def search_multimodal(query, top_k=TOP_K):
    q_emb = text_model.encode(query).astype("float32")
    if np.linalg.norm(q_emb)>0: q_emb/=np.linalg.norm(q_emb)

    res_audio,res_visual=[],[]

    # audio
    if audio_index and audio_index.ntotal>0:
        k = min(top_k, audio_index.ntotal)
        D,I = audio_index.search(q_emb.reshape(1,-1), k)
        for score, idx in zip(D[0], I[0]):
            if score >= AUDIO_SIM_THRESHOLD:
                m = audio_meta[idx]
                res_audio.append({**m,"score":float(score)})

    # visual
    if visual_index and visual_index.ntotal>0:
        k = min(top_k*3, visual_index.ntotal)
        D,I = visual_index.search(q_emb.reshape(1,-1), k)
        for score, idx in zip(D[0], I[0]):
            if score >= VISUAL_SIM_THRESHOLD:
                m = visual_meta[idx]
                res_visual.append({**m,"score":float(score)})

    # pick best
    best_audio = max(res_audio, key=lambda x:x["score"]) if res_audio else None
    best_visual = max(res_visual, key=lambda x:x["score"]) if res_visual else None

    if not best_audio and not best_visual:
        return {"found":False,"reason":"No match","audio_candidates":res_audio,"visual_candidates":res_visual}

    if best_audio and best_visual:
        winner = best_audio if best_audio["score"]>=best_visual["score"] else best_visual
        source = "audio" if winner==best_audio else "visual"
    elif best_audio: winner, source = best_audio, "audio"
    else: winner, source = best_visual, "visual"

    explanation = f"Matched {source} with score {winner['score']:.3f}"
    return {
        "found":True,
        "source":source,
        "score":winner["score"],
        "video":winner.get("video"),
        "video_id":winner.get("video_id"),
        "chunk_file":winner.get("chunk_file"),
        "start":winner.get("start"),
        "end":winner.get("end"),
        "transcript_or_caption":winner.get("transcript") if source=="audio" else winner.get("caption"),
        "timestamp":winner.get("timestamp") if source=="visual" else None,
        "explanation":explanation
    }

# ===============================
# MULTIMODAL QUERY HANDLERS
# ===============================

def process_text_query(query_text):
    """
    Text ‚Üí embedding ‚Üí multimodal search
    """
    result = search_multimodal(query_text)
    result["query_type"] = "text"
    return result


def process_image_query(image_path):
    """
    Image ‚Üí BLIP caption ‚Üí text embedding ‚Üí multimodal search
    """
    from PIL import Image

    image = Image.open(image_path).convert("RGB")

    inputs = blip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        out = blip_model.generate(**inputs, max_length=40)

    caption = blip_processor.decode(out[0], skip_special_tokens=True)

    print("üñºÔ∏è Image caption:", caption)

    result = search_multimodal(caption)
    result["query_type"] = "image"
    result["generated_caption"] = caption

    return result

def process_audio_query(audio_path):
    """
    Audio ‚Üí Whisper transcript ‚Üí text embedding ‚Üí multimodal search
    """
    try:
        res = whisper_model.transcribe(
            audio_path,
            fp16=False,
            language="en"
        )
        transcript = res.get("text", "").strip()
    except Exception as e:
        transcript = ""

    print("üéß Audio transcript:", transcript)

    if transcript == "":
        return {
            "found": False,
            "query_type": "audio",
            "reason": "Empty or failed audio transcription"
        }

    result = search_multimodal(transcript)
    result["query_type"] = "audio"
    result["generated_transcript"] = transcript

    return result

# ===============================
# UNIFIED MULTIMODAL ROUTER
# ===============================

def multimodal_query(
    text_query=None,
    image_path=None,
    audio_path=None
):
    """
    Automatically decides query modality
    """
    if text_query is not None:
        return process_text_query(text_query)

    if image_path is not None:
        return process_image_query(image_path)

    if audio_path is not None:
        return process_audio_query(audio_path)

    return {
        "found": False,
        "reason": "No valid query input provided"
    }

# ===============================
# TEST MULTIMODAL QUERIES
# ===============================

print("\n==============================")
print("üîé MULTIMODAL SEARCH TESTING")
print("==============================")

# TEXT QUERY
print("\n=== TEXT QUERY ===")
res = multimodal_query(text_query="crowd cheering in stadium")
print(res)

# IMAGE QUERY
print("\n=== IMAGE QUERY ===")
res = multimodal_query(image_path="/content/sample_query_image.jpg")
print(res)

# AUDIO QUERY
print("\n=== AUDIO QUERY ===")
res = multimodal_query(audio_path="/content/sample_query_audio.wav")
print(res)

import gradio as gr

# Function to process inputs based on user selection
def search_video(selected_inputs, text_query, image_file, audio_file):
    text_input = text_query if "Text" in selected_inputs else None
    image_input = image_file.name if "Image" in selected_inputs and image_file is not None else None
    audio_input = audio_file.name if "Audio" in selected_inputs and audio_file is not None else None

    if not (text_input or image_input or audio_input):
        return "‚ùå Please provide at least one input based on your selection."

    result = multimodal_query(
        text_query=text_input,
        image_path=image_input,
        audio_path=audio_input
    )

    if result["found"]:
        output_text = f"""
**Match found!**

**Similarity Score:** {result['score']:.3f}
**Source:** {result['source']}
**Video ID:** {result['video_id']}
**Chunk:** {result['chunk_file']}
**Time:** {result['start']}s - {result['end']}s
"""
        if "generated_caption" in result:
            output_text += f"\n**Image Caption:** {result['generated_caption']}\n"
        if "generated_transcript" in result:
            output_text += f"\n**Audio Transcript:** {result['generated_transcript']}\n"
        if "explanation" in result:
            output_text += f"\n**Explanation:** {result['explanation']}\n"
    else:
        output_text = f"‚ùå {result['reason']}"

    return output_text

# Dynamically update visibility of inputs
def update_inputs(selected_inputs):
    return (
        gr.update(visible="Text" in selected_inputs),
        gr.update(visible="Image" in selected_inputs),
        gr.update(visible="Audio" in selected_inputs)
    )

# Gradio Interface
with gr.Blocks() as demo:
    gr.Markdown("## üé• Multimodal Video Search Engine")
    gr.Markdown("Select which input(s) you want to provide: Text, Image, Audio")

    input_selector = gr.CheckboxGroup(
        ["Text", "Image", "Audio"],
        label="Select input type(s)"
    )

    text_input = gr.Textbox(label="Text Query", placeholder="Enter text query here...")
    image_input = gr.File(label="Upload Image", file_types=[".jpg", ".jpeg", ".png"])
    audio_input = gr.File(label="Upload Audio", file_types=[".wav", ".mp3", ".m4a"])
    output = gr.Markdown(label="Search Results")

    # Dynamically show/hide inputs
    input_selector.change(
        fn=update_inputs,
        inputs=input_selector,
        outputs=[text_input, image_input, audio_input]
    )

    search_btn = gr.Button("üîç Search")
    search_btn.click(
        fn=search_video,
        inputs=[input_selector, text_input, image_input, audio_input],
        outputs=output
    )

demo.launch(debug=True)

